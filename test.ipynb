{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 3.8361e-02,  7.0703e-01, -1.3202e-01,  ..., -9.7446e-02,\n",
       "           2.5193e-01,  3.4828e-01],\n",
       "         [ 2.1041e-01,  2.3984e-01,  9.1059e-03,  ..., -3.2353e-04,\n",
       "          -1.7492e-01,  4.0126e-02],\n",
       "         [ 2.3745e-01,  9.8414e-03, -1.6509e-01,  ..., -4.3378e-02,\n",
       "          -7.5783e-02,  4.6837e-02],\n",
       "         ...,\n",
       "         [ 2.3041e-01,  3.7583e-01,  1.7602e-02,  ...,  7.8472e-02,\n",
       "           1.8661e-01,  5.2053e-02],\n",
       "         [-2.3020e-01,  5.0276e-01,  1.0913e-01,  ..., -7.3260e-02,\n",
       "           1.4339e-01,  1.8320e-01],\n",
       "         [ 1.8813e-01,  6.2870e-01, -2.4809e-01,  ..., -4.8115e-02,\n",
       "           1.6404e-01,  4.7204e-01]]]), pooler_output=tensor([[ 5.9168e-02, -1.4942e-01,  3.2264e-02,  1.1025e-01, -9.1165e-02,\n",
       "          3.8922e-02, -1.7292e-01, -3.2285e-02,  1.3863e-01, -2.0987e-01,\n",
       "          3.2109e-01, -3.6521e-02,  1.0351e-01,  1.2799e-01,  4.0665e-02,\n",
       "          9.4652e-02, -7.3467e-02, -1.8222e-01, -1.7982e-01,  8.9638e-02,\n",
       "          4.2789e-03, -1.2962e-01,  2.0464e-01,  6.7162e-02,  2.8700e-02,\n",
       "          1.5024e-01,  2.7793e-01,  5.7419e-02, -3.3061e-02,  9.7894e-02,\n",
       "          1.5484e-01, -1.3220e-01, -3.1202e-02,  8.6422e-02, -3.7617e-03,\n",
       "          2.6688e-01, -6.9717e-02,  5.2877e-02, -1.1909e-01,  4.4180e-02,\n",
       "          1.7360e-01, -6.6465e-02,  1.8942e-01,  1.1936e-02, -1.8174e-01,\n",
       "         -8.0315e-02, -5.0086e-02,  1.0263e-01, -1.5271e-01, -2.1720e-01,\n",
       "          1.0207e-01,  2.3065e-01, -1.7684e-01,  1.0706e-01, -1.4842e-01,\n",
       "         -1.3789e-01, -1.3897e-01, -2.4128e-01,  9.0285e-02,  4.6982e-02,\n",
       "          1.6910e-01,  9.1170e-03, -2.0865e-01, -2.9226e-01,  4.1919e-02,\n",
       "         -1.8826e-01,  5.2114e-02, -1.4382e-01, -3.5226e-02,  1.4677e-01,\n",
       "         -5.8461e-02,  1.1320e-01, -8.3332e-03, -3.2913e-01, -1.7167e-02,\n",
       "         -4.7871e-01, -2.9590e-01,  9.0156e-03, -2.8186e-03, -3.2267e-02,\n",
       "          2.3646e-01, -5.2258e-02, -2.5985e-01,  1.4658e-01,  1.3886e-01,\n",
       "         -4.4353e-02,  1.2927e-03,  5.3436e-02,  3.4517e-01,  1.4137e-01,\n",
       "         -8.7021e-02,  9.4330e-02,  4.4854e-02,  7.0419e-02, -4.3536e-02,\n",
       "          1.3509e-01, -2.1375e-01, -2.9121e-01, -8.6180e-02, -1.0316e-01,\n",
       "         -5.3885e-02, -1.0690e-01,  1.1259e-01, -2.1095e-01,  2.6867e-02,\n",
       "          2.7575e-01,  4.2019e-02, -3.9910e-02, -1.5744e-01,  4.5616e-02,\n",
       "         -1.3530e-01,  5.6935e-02,  1.5504e-01, -1.2256e-01, -1.2447e-01,\n",
       "         -1.1539e-01,  1.6072e-02,  3.9108e-01, -4.1640e-02, -9.9614e-02,\n",
       "          3.8422e-02,  9.9262e-02,  2.9194e-02,  1.6251e-02,  7.1456e-02,\n",
       "         -2.7843e-02, -1.5923e-02,  2.5658e-01, -1.8091e-02, -9.4699e-02,\n",
       "         -1.8294e-01, -1.0915e-01,  9.0466e-02, -1.5617e-01,  4.6051e-02,\n",
       "          1.7854e-02,  5.5098e-02, -4.1778e-02,  2.9411e-01, -2.0995e-02,\n",
       "          4.3070e-02, -1.1326e-01,  2.3013e-01, -1.8173e-01, -1.1635e-01,\n",
       "         -9.5935e-02,  2.8391e-01, -2.1989e-01, -2.2761e-01, -1.9151e-02,\n",
       "         -1.6382e-01, -1.4781e-01,  6.2822e-02, -1.7556e-01, -2.8102e-02,\n",
       "          8.9866e-02, -1.1866e-02, -9.7714e-02,  2.0340e-01,  1.0988e-01,\n",
       "          7.3349e-02, -1.5432e-03,  4.5673e-02, -9.4467e-02, -1.1999e-01,\n",
       "          3.2382e-01,  8.3137e-02, -1.2118e-01, -1.1330e-01, -2.5361e-02,\n",
       "         -3.0557e-01, -9.2402e-02,  2.3103e-01,  6.8279e-03, -1.8195e-01,\n",
       "         -7.0096e-02,  6.8281e-02, -2.1587e-01,  1.1522e-01, -1.5088e-01,\n",
       "          4.2335e-02,  5.4473e-02, -1.1923e-01,  1.0849e-01, -1.8844e-01,\n",
       "         -4.4935e-02, -8.3844e-02, -2.5096e-02,  2.8213e-01,  7.2781e-02,\n",
       "         -1.5784e-01, -1.5986e-01, -1.4080e-01, -1.3631e-01,  2.9394e-01,\n",
       "         -1.0008e-03,  8.5472e-02, -3.2233e-02, -2.1359e-01,  1.1058e-01,\n",
       "         -8.6099e-02,  1.7632e-01, -9.2951e-02, -2.0127e-01, -1.2058e-01,\n",
       "         -5.2684e-02, -7.6966e-02,  5.7964e-02,  1.4025e-01,  1.9744e-01,\n",
       "         -5.1772e-02,  2.3590e-01, -7.8975e-02,  1.0743e-01,  9.1483e-02,\n",
       "         -5.6561e-03,  3.8608e-02,  5.6066e-02,  1.0550e-01,  1.2460e-01,\n",
       "          8.1108e-02,  2.5319e-01, -9.2112e-02, -3.3283e-02, -6.2467e-02,\n",
       "          1.5519e-01, -5.8339e-02,  8.7733e-03,  1.7489e-02,  1.4714e-01,\n",
       "         -5.5524e-02,  7.9027e-03,  3.8704e-02, -1.6401e-01,  1.4842e-02,\n",
       "         -2.4206e-01,  1.2379e-01, -1.4772e-01,  8.2105e-03, -8.5691e-02,\n",
       "          5.1752e-02, -1.9323e-03,  1.5053e-01, -6.2370e-02, -4.1476e-02,\n",
       "          1.1300e-01,  2.7851e-01, -1.4188e-01,  2.8574e-02,  1.4368e-01,\n",
       "         -9.6436e-02,  1.6587e-02, -1.1922e-01,  8.9029e-02,  2.0887e-01,\n",
       "          4.9972e-03, -1.4939e-01, -1.4390e-01, -1.3849e-01, -1.0398e-01,\n",
       "          1.0544e-01,  1.2757e-01,  2.5134e-01,  3.2834e-02,  2.6179e-01,\n",
       "          3.6108e-01,  5.5661e-02,  6.3866e-02, -1.1614e-01, -1.2601e-01,\n",
       "         -3.0825e-01, -3.0401e-02, -5.9941e-02,  8.2088e-03,  2.3578e-02,\n",
       "          6.6635e-02, -1.3898e-02,  1.2596e-03,  3.7048e-03,  8.8162e-02,\n",
       "         -4.3669e-02, -7.5682e-02,  1.5982e-02,  2.8567e-01, -1.5456e-01,\n",
       "         -9.9965e-02,  2.0210e-01, -2.4539e-01, -1.4460e-01,  1.0925e-01,\n",
       "         -1.8411e-01, -9.1114e-02, -1.9171e-01,  1.4902e-01,  1.6527e-01,\n",
       "          8.5615e-02, -7.5334e-02, -1.5743e-01, -1.7121e-01,  7.5554e-02,\n",
       "          2.3941e-02,  1.9633e-01,  1.0803e-01, -6.6198e-03,  1.3147e-01,\n",
       "          2.3923e-01, -1.3105e-01,  1.4035e-01,  1.9049e-01,  5.6838e-02,\n",
       "          1.4450e-01,  2.5872e-02, -2.1692e-01, -3.9875e-02, -8.2234e-02,\n",
       "         -1.5494e-01,  7.0216e-02, -3.5072e-02,  6.9157e-02,  3.9879e-02,\n",
       "         -1.9171e-01, -1.2833e-01, -5.4461e-02,  2.1453e-01,  5.6738e-02,\n",
       "          3.7432e-02,  1.4678e-01, -9.2613e-03,  5.4628e-02,  2.5934e-01,\n",
       "         -6.9107e-02, -3.0698e-02, -1.4730e-01, -7.0408e-02,  1.4091e-02,\n",
       "         -5.8311e-02, -9.2268e-02,  1.4734e-03,  3.5473e-01, -9.1069e-02,\n",
       "          1.0882e-01, -4.7398e-02,  5.0343e-02, -7.2714e-02, -7.1328e-03,\n",
       "         -9.2345e-02,  1.7509e-01,  4.5715e-02,  8.6065e-02, -1.5164e-02,\n",
       "         -9.3171e-03, -8.3584e-02,  2.2538e-01, -1.1490e-01, -7.7893e-02,\n",
       "          1.2554e-01, -1.0565e-01, -5.7960e-02,  9.9371e-02,  1.1541e-02,\n",
       "         -3.1896e-01, -1.3138e-02,  7.5872e-02, -6.9832e-03,  1.7878e-01,\n",
       "          2.8468e-02,  2.6043e-01, -1.8479e-01, -1.3395e-01, -3.9560e-02,\n",
       "          1.2050e-01, -6.2636e-02, -6.5039e-02,  1.3023e-01, -1.3017e-01,\n",
       "          1.5265e-01, -8.5207e-02,  6.0715e-02,  1.8470e-01,  9.0338e-02,\n",
       "         -1.9660e-01, -7.2169e-03,  1.2420e-01,  3.9175e-02,  6.2451e-02,\n",
       "          1.2356e-01, -7.9792e-02,  3.1128e-02, -1.0809e-02, -6.7345e-02,\n",
       "         -3.1197e-01, -1.9641e-01,  1.8145e-02,  1.4853e-02,  2.2239e-01,\n",
       "          9.3292e-03,  6.5147e-02, -1.4948e-01,  6.0140e-02,  3.0149e-01,\n",
       "         -7.4888e-02, -2.3143e-01, -1.1251e-01,  2.2837e-01, -4.2280e-02,\n",
       "          7.0855e-02,  1.5377e-01,  1.6623e-01, -7.8639e-02, -2.6835e-02,\n",
       "         -1.0434e-03,  9.1999e-02,  1.3804e-01, -1.7635e-02,  2.7600e-02,\n",
       "         -2.1494e-01,  5.0354e-02, -1.1015e-01,  9.4447e-02, -3.1574e-01,\n",
       "         -7.6261e-02, -2.2242e-01, -3.2296e-01, -2.2727e-01, -1.1739e-01,\n",
       "         -7.8047e-02, -1.0353e-01,  2.0612e-01, -6.2022e-02,  2.1383e-01,\n",
       "          2.4203e-01,  1.1790e-01, -9.1747e-02, -5.4947e-02,  2.6491e-01,\n",
       "         -3.1380e-02,  4.3871e-01,  6.6768e-02,  5.4414e-02,  1.0233e-01,\n",
       "         -1.0320e-01, -8.3988e-02, -2.4572e-02, -1.2921e-01, -1.5762e-03,\n",
       "         -3.7769e-02,  3.1501e-04, -1.0833e-01, -1.5682e-01, -3.0221e-01,\n",
       "         -1.1858e-01, -7.0899e-02,  2.6225e-01,  1.1380e-01, -2.7259e-02,\n",
       "         -2.6463e-02, -2.2665e-01,  2.9271e-01,  1.6304e-01, -1.5025e-02,\n",
       "         -8.0336e-02, -1.3476e-01, -4.3643e-02,  1.6332e-02, -9.6058e-02,\n",
       "          8.1625e-02, -1.1252e-01, -1.0932e-01, -1.7536e-01, -4.0012e-02,\n",
       "          2.0216e-01,  1.4785e-01,  1.3661e-01, -2.9894e-01, -6.8547e-02,\n",
       "         -4.2566e-02,  1.3405e-01, -5.2619e-02, -1.6542e-02,  1.4007e-01,\n",
       "         -3.7639e-01,  7.3813e-02,  2.4918e-01, -1.5130e-01, -1.5002e-01,\n",
       "         -1.9943e-01,  1.0863e-01, -8.1459e-02, -4.1885e-01,  1.3801e-01,\n",
       "          3.8609e-02, -5.7068e-03, -9.9520e-02, -2.8163e-01, -1.9495e-01,\n",
       "          1.5852e-02, -1.1806e-01, -3.1094e-02,  1.4126e-01,  5.5042e-03,\n",
       "          1.6744e-02, -1.6402e-01, -2.1960e-02,  1.5374e-03,  2.4338e-01,\n",
       "         -2.7166e-01, -1.8324e-01, -3.1083e-02, -1.6887e-01, -1.1295e-01,\n",
       "          2.5420e-01, -2.4861e-01, -2.4223e-01,  3.7503e-01,  1.3881e-01,\n",
       "         -2.4690e-01, -2.8729e-01,  3.3075e-02,  3.6425e-01,  1.1110e-01,\n",
       "          9.1037e-02,  4.6775e-02, -4.8824e-02, -1.4983e-01,  1.1551e-01,\n",
       "          4.2832e-01, -1.5820e-01,  4.6238e-02, -2.7623e-02, -8.8341e-02,\n",
       "          2.3278e-02,  1.2403e-01, -1.7130e-01, -1.6071e-01, -2.3658e-01,\n",
       "          1.2265e-01, -4.3343e-02, -1.4084e-01,  4.4265e-02, -1.8835e-01,\n",
       "         -4.4789e-02, -1.8870e-01, -8.3775e-02,  1.9592e-01,  1.7699e-01,\n",
       "         -3.6275e-01, -1.5680e-02,  1.4866e-01,  4.5300e-03,  4.0587e-02,\n",
       "          1.8425e-02,  1.8963e-01,  3.2069e-03,  3.8150e-02, -4.7287e-02,\n",
       "         -1.0761e-01,  5.5656e-02,  4.1983e-01,  1.0337e-01, -2.4961e-02,\n",
       "         -1.1168e-01,  3.8896e-02, -1.2431e-01,  5.7095e-02, -7.7847e-02,\n",
       "         -1.1824e-01, -5.5114e-02,  1.8409e-01, -2.8431e-02,  3.2985e-01,\n",
       "          2.2687e-01, -3.7251e-03,  1.4065e-01,  1.8123e-03,  1.0859e-01,\n",
       "         -1.9167e-01, -2.6242e-01, -7.3837e-02,  4.8574e-02, -2.4150e-01,\n",
       "          2.0551e-02,  2.2451e-02, -2.6553e-01, -1.6287e-01, -2.6810e-01,\n",
       "          9.9639e-03, -4.9691e-02, -1.1553e-02,  6.7604e-02,  1.5589e-01,\n",
       "         -8.6800e-02, -1.0440e-01,  1.5892e-02,  3.3811e-01, -9.0158e-02,\n",
       "          2.2223e-01,  1.8573e-01, -8.4965e-02,  7.2521e-02, -1.6057e-01,\n",
       "         -4.3971e-02, -1.6187e-01, -1.7205e-01,  3.5079e-02,  7.7990e-02,\n",
       "         -1.0024e-01, -9.5227e-02,  2.9743e-02, -2.0468e-01,  3.4933e-02,\n",
       "          7.2275e-02, -1.0644e-01,  8.7189e-03,  1.1093e-01, -9.3671e-02,\n",
       "         -8.9970e-02,  1.7250e-01, -9.7212e-02,  9.0268e-02,  1.4730e-01,\n",
       "          1.0285e-01,  1.8950e-01,  3.0973e-01,  8.9311e-02, -2.4838e-01,\n",
       "          1.0384e-01,  2.1649e-02,  2.7118e-01, -4.2444e-02,  6.7715e-02,\n",
       "         -3.7359e-03,  6.8126e-02,  1.3519e-01, -2.2873e-01,  2.9268e-01,\n",
       "          1.0703e-01,  3.3599e-01,  9.1387e-02,  4.1837e-01,  2.5252e-01,\n",
       "          1.1299e-01,  1.0516e-01, -1.3653e-01, -1.9271e-01, -7.8002e-02,\n",
       "          1.2190e-01, -1.9676e-01,  1.1604e-01,  2.0573e-01,  8.4451e-02,\n",
       "          7.1781e-03, -5.8971e-02, -4.7379e-02, -2.1148e-01, -3.4803e-01,\n",
       "         -1.0388e-01, -4.6614e-02, -3.7333e-02,  1.1221e-01, -1.1542e-01,\n",
       "         -1.4371e-01,  3.0113e-02, -3.9956e-02, -2.3287e-01,  1.8788e-01,\n",
       "          1.8949e-01,  6.1309e-02,  1.9303e-01,  3.9470e-02, -8.9989e-04,\n",
       "          2.3082e-01,  1.0220e-01, -2.9798e-01, -9.7038e-02,  1.1058e-01,\n",
       "         -7.2700e-02, -4.9077e-02, -1.1201e-01, -8.6995e-02, -7.6801e-02,\n",
       "         -1.2988e-01,  4.9307e-02, -2.3956e-01, -1.0095e-01,  1.6314e-01,\n",
       "          7.6627e-02,  1.3767e-01,  1.9028e-01,  9.2963e-03,  6.8701e-02,\n",
       "         -3.4467e-02,  7.7501e-02,  9.3847e-02,  3.6207e-02,  4.2166e-02,\n",
       "         -8.1680e-02,  2.1749e-02, -1.9138e-02,  1.9313e-01,  2.2655e-03,\n",
       "          2.8230e-01, -5.2198e-02,  1.7183e-02, -1.1233e-01, -1.1050e-01,\n",
       "          1.9375e-01, -2.9642e-01,  3.2952e-01, -8.6636e-02,  1.1566e-01,\n",
       "          2.1525e-01, -5.3940e-02, -1.5494e-01, -1.3478e-01,  9.7815e-02,\n",
       "         -6.4257e-03,  6.5358e-02, -8.9629e-02,  9.8131e-02, -1.2891e-01,\n",
       "          7.4047e-02,  3.1646e-01,  1.7358e-01, -7.5929e-02,  9.7088e-02,\n",
       "         -2.5193e-02, -2.1994e-01,  2.8149e-01,  9.5130e-02,  7.9587e-02,\n",
       "          7.6939e-02,  1.2312e-01, -3.8769e-02, -6.0065e-02, -6.2371e-02,\n",
       "          1.7891e-01, -4.5641e-02,  1.8439e-02,  1.9995e-01, -1.2557e-01,\n",
       "         -1.7756e-01, -6.9150e-02,  2.7232e-01, -1.8112e-03, -2.9180e-02,\n",
       "          1.0112e-01, -1.4173e-01, -2.3598e-02,  2.8573e-01, -1.3120e-01,\n",
       "          6.5104e-03,  3.8420e-02,  5.2870e-02,  1.0888e-01, -1.0477e-02,\n",
       "          2.0011e-01,  2.9077e-01,  1.7382e-01, -7.5971e-02, -1.1946e-01,\n",
       "         -1.4256e-01, -5.3187e-02,  3.0707e-01, -1.1394e-01,  1.5969e-01,\n",
       "          1.6687e-01,  8.5336e-02,  8.9997e-02]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "phobert = AutoModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!\n",
    "sentence = 'Chúng_tôi là những nghiên_cứu_viên .'  \n",
    "\n",
    "input_ids = torch.tensor([tokenizer.encode(sentence)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = phobert(input_ids)  # Models outputs are now tuples\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup input process and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from underthesea import word_tokenize, text_normalize\n",
    "\n",
    "# text cleaning\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđĐ ]')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "# text nomalization\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def standardize_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html_tags(text)\n",
    "    text = remove_non_alpha(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = standardize_spaces(text)\n",
    "    text = text_normalize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chúng_tôi là những nghiên_cứu_viên check out đây là một ví_dụ\n"
     ]
    }
   ],
   "source": [
    "text = \"Chúng tôi là những nghiên cứu viên. Check out: https://example.com <br> Đây là một ví dụ!\"\n",
    "cleaned_text = preprocess_text(text)\n",
    "tokens = word_tokenize(cleaned_text, format=\"text\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "path_dataset = \"dataset.csv\"\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(path_dataset)\n",
    "\n",
    "# Convert all entries in the 'Text' column to strings\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "\n",
    "\n",
    "# Preprocess the 'text' column\n",
    "df['processed_text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "df['word_tokens'] = df['processed_text'].apply(lambda x: word_tokenize(x, format=\"text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       theo sankei sports sáng nay thủ_tướng nhật_bản...\n",
       "1       nhìn bức tranh bạn có_thể thấy vòng_tròn đang ...\n",
       "2       sau khi vụ án xảy ra tất_cả mọi người đều khôn...\n",
       "3       xét_nghiệm ban_đầu từ các trẻ nhập_viện cho th...\n",
       "4       sáng ngày 289 theo nguồn tin riêng từ ubnd thà...\n",
       "                              ...                        \n",
       "1901    mẫu xe hạng a của thương_hiệu hàn_quốc nhận đư...\n",
       "1902    theo hđxx bị_cáo ngô_minh khâm_giữ vai_trò chủ...\n",
       "1903    cổ động_viên real_madrid bất_mãn khi hậu_vệ da...\n",
       "1904    một bác_sĩ trong êkíp đỡ_sinh phải bế và tỳ_cả...\n",
       "1905    thay_vì nỗ_lực kiếm tiền nhiều người trẻ mới t...\n",
       "Name: word_tokens, Length: 1906, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the processed data\n",
    "df['word_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base-v2\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"batch_size\": 8,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Load the configuration of the model\n",
    "config = AutoConfig.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "# Specify the number of labels for sequence classification\n",
    "config.num_labels = 2\n",
    "config.batch_size = 8 \n",
    "\n",
    "# Print the maximum input length\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from underthesea import word_tokenize, text_normalize\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "sentences = list(df['word_tokens'])\n",
    "\n",
    "\n",
    "labels = list(df['Label'])\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Split the data into training and validation sets while maintaining the label distribution\n",
    "train_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_labels, val_labels = train_test_split(\n",
    "    inputs.input_ids, inputs.attention_mask, torch.tensor(labels), test_size=0.2, random_state=42, stratify=torch.tensor(labels)\n",
    ")\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the training set: 96\n",
      "Number of batches in the training set: 24\n",
      "Number of samples in the training set: 1524\n",
      "Number of samples in the validation set: 382\n"
     ]
    }
   ],
   "source": [
    "# For the training dataloader\n",
    "train_samples = len(train_dataloader.dataset)\n",
    "\n",
    "# For the validation dataloader\n",
    "val_samples = len(val_dataloader.dataset)\n",
    "\n",
    "print(f\"Number of batches in the training set: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in the training set: {len(val_dataloader)}\")\n",
    "\n",
    "print(f\"Number of samples in the training set: {train_samples}\")\n",
    "print(f\"Number of samples in the validation set: {val_samples}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for customer fine tunning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CustomClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_size, num_labels):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "# Replace the classification head in PhoBERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base-v2\", num_labels=2)\n",
    "model.classifier = CustomClassificationHead(model.config.hidden_size, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use default setup for fine tunning layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 258]) torch.Size([16, 258]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in train_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 258]) torch.Size([16, 258]) torch.Size([16])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\KLTN_official\\test.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m input_ids, attention_mask, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(input_ids\u001b[39m.\u001b[39mshape, attention_mask\u001b[39m.\u001b[39mshape, labels\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1196\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1196\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1197\u001b[0m     input_ids,\n\u001b[0;32m   1198\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1199\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1200\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1201\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1202\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1203\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1204\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1205\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1206\u001b[0m )\n\u001b[0;32m   1207\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1208\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:837\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    835\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 837\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m    838\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    839\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    840\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    841\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    842\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m    843\u001b[0m )\n\u001b[0;32m    844\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    845\u001b[0m     embedding_output,\n\u001b[0;32m    846\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    854\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    856\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:130\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    128\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embeddings(position_ids)\n\u001b[0;32m    131\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[0;32m    132\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\functional.py:2043\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2037\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2038\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2043\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model using the modified configuration\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base-v2\", config=config)\n",
    "\n",
    "model.train()\n",
    "for batch in train_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    print(input_ids.shape, attention_mask.shape, labels.shape)\n",
    "    print(model(input_ids, attention_mask=attention_mask, labels=labels))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\KLTN_official\\test.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m input_ids, attention_mask, labels \u001b[39m=\u001b[39m batch\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/KLTN_official/test.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1196\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1196\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1197\u001b[0m     input_ids,\n\u001b[0;32m   1198\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1199\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1200\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1201\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1202\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1203\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1204\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1205\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1206\u001b[0m )\n\u001b[0;32m   1207\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1208\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:837\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    835\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 837\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m    838\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    839\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    840\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    841\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    842\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m    843\u001b[0m )\n\u001b[0;32m    844\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    845\u001b[0m     embedding_output,\n\u001b[0;32m    846\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    854\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    855\u001b[0m )\n\u001b[0;32m    856\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:130\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    128\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabsolute\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 130\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mposition_embeddings(position_ids)\n\u001b[0;32m    131\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[0;32m    132\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\Hi\\anaconda3\\envs\\KLTN\\lib\\site-packages\\torch\\nn\\functional.py:2043\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2037\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2038\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2039\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2040\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2043\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model using the modified configuration\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base-v2\", config=config)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 4\n",
    "\n",
    "# Assuming you have a validation dataloader named val_dataloader\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # Training\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        model.val()\n",
    "        for batch in val_dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_predictions.double() / len(val_dataloader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Validation Loss: {avg_val_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    new_sentence = \"This is a new article to check.\"\n",
    "    inputs = tokenizer(new_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    logits = model(**inputs).logits\n",
    "    prediction = torch.argmax(logits, dim=1).item()\n",
    "    print(\"Fake\" if prediction == 1 else \"Real\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KLTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
